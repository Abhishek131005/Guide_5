# --- Step 1: Import Libraries and Load Data ---

import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

# This line will open an "Upload" button for you to choose your file
print("Please upload your dataset (CSV file):")
uploaded = files.upload()

# Get the name of the file you just uploaded
file_name = list(uploaded.keys())[0]

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_name)

print(f"\nSuccessfully loaded '{file_name}'")
print("----------------------------------------")


# --- Step 2: Data Inspection ---
# We look at the data to understand all features.
# K-Means is unsupervised, so there is NO target column (y).

print("Step 2: Data Inspection")
print("First 5 rows of the data:")
print(df.head())

print("\nColumn names and data types:")
df.info()
print("----------------------------------------")


# --- Step 3: Data Preprocessing ---

print("Step 3: Data Preprocessing")

# --- 3a. Handle Categorical Data (One-Hot Encoding) ---
# We must convert all text columns to numbers.
X = pd.get_dummies(df)
print("Categorical columns have been one-hot encoded.")

# --- 3b. Handle Missing Data (Imputation) ---
# Fill any missing values with the mean of their column
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)
print("Missing data has been filled with the mean.")

# --- 3c. Feature Scaling ---
# This is THE MOST IMPORTANT step for K-Means.
# It ensures all features are on the same scale (distance-based).
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("All features have been scaled.")
print("\nPreprocessed Data (first 5 rows):")
print(X_scaled[:5])
print("----------------------------------------")


# --- Step 4: Find the Optimal 'k' (The Elbow Method) ---

print("Step 4: Finding the Optimal 'k' using the Elbow Method")

# WCSS = Within-Cluster Sum of Squares
wcss = []
k_range = range(1, 11)  # We will test k from 1 to 10

for k in k_range:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_) # .inertia_ gives the WCSS

# Plot the Elbow graph
plt.figure(figsize=(8, 5))
plt.plot(k_range, wcss, 'bo-')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters (k)')
plt.ylabel('WCSS (Inertia)')
plt.xticks(k_range)
plt.show()

print("\nLook at the plot above. The 'elbow' is the point")
print("where the drop in WCSS slows down. This is your")
print("best guess for the optimal 'k' value.")
print("----------------------------------------")


# --- Step 5: Build and Train the Final K-Means Model ---

print("Step 5: Building and Training the Final K-Means Model")

# !!! IMPORTANT: YOU MUST EDIT THIS LINE !!!
# Change 'K_VALUE' to the 'k' you found from the Elbow plot.
# For example, if the elbow is at 3, set K_VALUE = 3.
K_VALUE = 3
# (This is just a default, change it!)

print(f"Building model with k={K_VALUE}...")

# Initialize and train the model
# 'init='k-means++'' is a smart way to initialize centers
# 'n_init=10' runs the algorithm 10 times and picks the best one
model = KMeans(n_clusters=K_VALUE, init='k-means++', n_init=10, random_state=42)

# fit_predict finds the clusters and returns the cluster label for each point.
labels = model.fit_predict(X_scaled)

print("Model training complete.")
print("----------------------------------------")


# --- Step 6: Evaluate the Clustering ---

print("Step 6: Evaluating the Clustering")

# Get the coordinates of the cluster centers
centers = model.cluster_centers_
print(f"Found {len(centers)} cluster centers.")

# Calculate Silhouette Score
score = silhouette_score(X_scaled, labels)
print(f"\nSilhouette Score: {score:.4f}")
print("(Score is from -1 to 1. Higher is better.)")

print("----------------------------------------")


# --- Step 7: Visualize the Results (using PCA) ---

print("Step 7: Visualizing the Clusters")

# K-Means can work in many dimensions, but we can only plot in 2D.
# We use PCA to reduce the data to 2 dimensions for visualization.
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# We also need to transform the cluster centers to the same 2D space
centers_pca = pca.transform(centers)

plt.figure(figsize=(10, 7))

# Plot each cluster.
# We create a scatter plot, coloring each point by its cluster label.
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)

# Plot the cluster centers
plt.scatter(centers_pca[:, 0], centers_pca[:, 1],
            marker='X', s=200, linewidths=3,
            color='red', zorder=10)

plt.title(f"K-Means Clustering with k={K_VALUE} (PCA-reduced)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label='Cluster Label')
plt.show()

print("\nVisualization Complete.")
print("The red 'X' marks are the centers of each cluster.")