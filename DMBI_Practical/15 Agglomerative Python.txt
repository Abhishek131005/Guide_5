# --- Step 1: Import Libraries and Load Data ---

import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files

from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

# Import scipy for the dendrogram
import scipy.cluster.hierarchy as sch

# This line will open an "Upload" button for you to choose your file
print("Please upload your dataset (CSV file):")
uploaded = files.upload()

# Get the name of the file you just uploaded
file_name = list(uploaded.keys())[0]

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_name)

print(f"\nSuccessfully loaded '{file_name}'")
print("----------------------------------------")


# --- Step 2: Data Inspection ---
# We look at the data to understand all features.
# This is unsupervised, so there is NO target column (y).

print("Step 2: Data Inspection")
print("First 5 rows of the data:")
print(df.head())

print("\nColumn names and data types:")
df.info()
print("----------------------------------------")


# --- Step 3: Data Preprocessing ---

print("Step 3: Data Preprocessing")

# --- 3a. Handle Categorical Data (One-Hot Encoding) ---
X = pd.get_dummies(df)
print("Categorical columns have been one-hot encoded.")

# --- 3b. Handle Missing Data (Imputation) ---
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)
print("Missing data has been filled with the mean.")

# --- 3c. Feature Scaling ---
# This is THE MOST IMPORTANT step.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("All features have been scaled.")
print("\nPreprocessed Data (first 5 rows):")
print(X_scaled[:5])
print("----------------------------------------")


# --- Step 4: Find the Optimal 'k' (The Dendrogram) ---

print("Step 4: Finding the Optimal 'k' using the Dendrogram")

# Agglomerative clustering is "bottom-up". It starts with every
# point as its own cluster and merges them up.
# The Dendrogram shows this entire hierarchy.

print("Generating dendrogram... this may take a moment.")
plt.figure(figsize=(15, 7))

# We use 'ward' linkage. This method tries to minimize the
# variance when merging clusters.
# 'linkage' calculates the distances and hierarchy.
linked = sch.linkage(X_scaled, method='ward')

# 'dendrogram' plots the hierarchy.
# We 'truncate' it to show only the last 5 levels (p=5)
# to keep it readable, otherwise 1000+ points are too messy.
sch.dendrogram(linked,
               truncate_mode='level',
               p=5)

plt.title('Hierarchical Clustering Dendrogram (Truncated)')
plt.xlabel('Cluster Size')
plt.ylabel('Distance')
plt.show()

print("\n--- How to Read the Dendrogram ---")
print("1. Look for the longest vertical lines that are not crossed")
print("   by any horizontal lines.")
print("2. Imagine drawing a horizontal line that cuts these")
print("   longest lines.")
print("3. The number of vertical lines you just crossed is the")
print("   optimal number of clusters (k).")
print("----------------------------------------")


# --- Step 5: Build and Train the Final Model ---

print("Step 5: Building and Training the Final Agglomerative Model")

# !!! IMPORTANT: YOU MUST EDIT THIS LINE !!!
# Change 'K_VALUE' to the 'k' you found from the Dendrogram.
# For example, if you count 3 vertical lines, set K_VALUE = 3.
K_VALUE = 3
# (This is just a default, change it!)

print(f"Building model with k={K_VALUE}...")

# Initialize and train the model
# metric='euclidean' is the distance metric
# linkage='ward' is the merge strategy
model = AgglomerativeClustering(n_clusters=K_VALUE,
                                metric='euclidean',
                                linkage='ward')

# fit_predict finds the clusters and returns the label for each point.
labels = model.fit_predict(X_scaled)

print("Model training complete.")
print("----------------------------------------")


# --- Step 6: Evaluate the Clustering ---

print("Step 6: Evaluating the Clustering")

# Calculate Silhouette Score
score = silhouette_score(X_scaled, labels)
print(f"\nSilhouette Score: {score:.4f}")
print("(Score is from -1 to 1. Higher is better.)")

print("----------------------------------------")


# --- Step 7: Visualize the Results (using PCA) ---

print("Step 7: Visualizing the Clusters")

# We use PCA to reduce the data to 2 dimensions for visualization.
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 7))

# Plot each cluster.
# We create a scatter plot, coloring each point by its cluster label.
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)

plt.title(f"Agglomerative Clustering with k={K_VALUE} (PCA-reduced)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label='Cluster Label')
plt.show()

print("\nVisualization Complete.")
print("Unlike K-Means, there are no 'centroids' to plot.")